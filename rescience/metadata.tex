% DO NOT EDIT - automatically generated from metadata.yaml

\def \codeURL{https://github.com/KlemenVovk/noisy-labels}
\def \codeDOI{}
\def \codeSWH{}
\def \dataURL{}
\def \dataDOI{}
\def \editorNAME{}
\def \editorORCID{}
\def \reviewerINAME{}
\def \reviewerIORCID{}
\def \reviewerIINAME{}
\def \reviewerIIORCID{}
\def \dateRECEIVED{01 November 2018}
\def \dateACCEPTED{}
\def \datePUBLISHED{}
\def \articleTITLE{Learning with Noisy Labels [\textasciitilde Re]visited}
\def \articleTYPE{Replication}
\def \articleDOMAIN{Machine Learning}
\def \articleBIBLIOGRAPHY{bibliography.bib}
\def \articleYEAR{2025}
\def \reviewURL{https://github.com/ReScience/submissions/issues/86}
\def \articleABSTRACT{Learning with noisy labels (LNL) is a subfield of supervised machine learning investigating scenarios in which the training data contain errors. While most research has focused on synthetic noise, where labels are randomly corrupted, real-world noise from human annotation errors is more complex and less understood. Wei et al. (2022) introduced CIFAR-N, a dataset with human-labeled noise and claimed that real-world noise is fundamentally more challenging than synthetic noise. This study aims to reproduce their experiments on testing the characteristics of human-annotated label noise, memorization dynamics, and benchmarking of LNL methods. We successfully reproduce some of the claims but identify some quantitative discrepancies. Notably, our attempts to reproduce the reported benchmark reveal inconsistencies in the reported results. To address these issues, we develop a unified framework and propose a refined benchmarking protocol that ensures a fairer evaluation of LNL methods. Our findings confirm that real-world noise differs structurally from synthetic noise and is memorized more rapidly by deep networks. By open-sourcing our implementation, we provide a more reliable foundation for future research in LNL.}
\def \replicationCITE{WEI, Jiaheng, et al. Learning with noisy labels revisited: A study using real-world human annotations. arXiv preprint arXiv:2110.12088, 2021.}
\def \replicationBIB{noisylabels-benchmark}
\def \replicationURL{https://openreview.net/pdf?id=TBWA6PLJZQm}
\def \replicationDOI{https://doi.org/10.48550/arXiv.2110.12088}
\def \contactNAME{Valter Hudovernik}
\def \contactEMAIL{vh0153@student.uni-lj.si}
\def \articleKEYWORDS{Learning with noisy labels, deep learning, Python}
\def \journalNAME{ReScience C}
\def \journalVOLUME{4}
\def \journalISSUE{1}
\def \articleNUMBER{}
\def \articleDOI{}
\def \authorsFULL{Valter Hudovernik et al.}
\def \authorsABBRV{V. Hudovernik et al.}
\def \authorsSHORT{Hudovernik et al.}
\title{\articleTITLE}
\date{}
\author[1,\orcid{0000-0003-3045-521X}]{Valter Hudovernik}
\author[1,\orcid{0009-0001-0524-5221}]{Žiga Rot}
\author[1,\orcid{0009-0001-0543-484X}]{Klemen Vovk}
\author[1,\orcid{0009-0008-0188-8757}]{Luka Škodnik}
\author[1,\orcid{0000-0003-2823-272X}]{Luka Čehovin Zajc}
\affil[1]{University of Ljubljana, Faculty of Computer and Information Science, Večna pot 113, 1000 Ljubljana}
